{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aphilak660510747/229352-stat/blob/main/Lab04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9252fd0-8fb3-4237-a32c-d2ae904aeba1",
      "metadata": {
        "id": "e9252fd0-8fb3-4237-a32c-d2ae904aeba1"
      },
      "source": [
        "### Statistical Learning for Data Science 2 (229352)\n",
        "#### Instructor: Donlapark Ponnoprat\n",
        "\n",
        "#### [Course website](https://donlapark.pages.dev/229352/)\n",
        "\n",
        "## Lab #4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "70b30d42-2935-4d97-afb4-51b444e01360",
      "metadata": {
        "id": "70b30d42-2935-4d97-afb4-51b444e01360"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from scipy.stats import uniform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "b4116b7d-ca3e-4e82-8452-ec6b220bb328",
      "metadata": {
        "id": "b4116b7d-ca3e-4e82-8452-ec6b220bb328",
        "outputId": "c13471f7-2a95-42f6-af0b-f2431ed31043",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X: 500\n",
            "y: 500\n"
          ]
        }
      ],
      "source": [
        "train = fetch_20newsgroups(subset='train')\n",
        "test = fetch_20newsgroups(subset='test')\n",
        "\n",
        "Xtrain = train.data[:3000]\n",
        "ytrain = train.target[:3000]\n",
        "Xtest = test.data[:500]\n",
        "ytest = test.target[:500]\n",
        "\n",
        "print(\"X:\", len(Xtest))\n",
        "print(\"y:\", len(ytest))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "452ca0da-2658-4daa-85be-d42ea298fe07",
      "metadata": {
        "id": "452ca0da-2658-4daa-85be-d42ea298fe07"
      },
      "source": [
        "### Naive Bayes [(Documentation)](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "1a2cb016-1e96-44e8-8c64-823ad825afd9",
      "metadata": {
        "id": "1a2cb016-1e96-44e8-8c64-823ad825afd9"
      },
      "outputs": [],
      "source": [
        "pipe = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('nb', MultinomialNB())\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90fe0d6a-bb47-40ba-8660-c1af8f35eeb9",
      "metadata": {
        "id": "90fe0d6a-bb47-40ba-8660-c1af8f35eeb9"
      },
      "source": [
        "### Random Search Cross-Validation [(Documentation)](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n",
        "\n",
        "### Uniform distribution in `Scipy` [(Documentation)](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.uniform.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "73f44229-8e90-40ad-b1e7-827597dba207",
      "metadata": {
        "id": "73f44229-8e90-40ad-b1e7-827597dba207",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "a7e0a530-3224-42ea-e897-295425d2ef20"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (ipython-input-30-2274929817.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-30-2274929817.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    random_search = RandomizedSearchCV(\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "74f7ad65-0b56-4987-9493-c5f06a7f481b",
      "metadata": {
        "id": "74f7ad65-0b56-4987-9493-c5f06a7f481b"
      },
      "source": [
        "#### Exercise\n",
        "\n",
        "1. For the Naive Bayes model, use grid search 5-fold cross-validation across different values of `alpha` to find the best model.\n",
        "\n",
        "2. For the best value of `alpha`, compute the `f1_macro` score on the test set.\n",
        "* What value of `alpha` did you obtain?\n",
        "* What is the model's `f1_macro` score?\n",
        "\n",
        "3. Repeat Exercise 1 and 2 for **random search** 5-fold cross validation across different values of `alpha`. Compute the `f1_macro` score on the test set.\n",
        "* What value of `alpha` did you obtain?\n",
        "* Did you get a better `f1_macro` score compared to grid search in Exercise 2?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "81b62e5d-b9f9-49a9-b3dc-0317cad6b6df",
      "metadata": {
        "id": "81b62e5d-b9f9-49a9-b3dc-0317cad6b6df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a70fbc6-a0bf-4812-96f8-d4fb0051ea02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading 20 Newsgroups dataset...\n",
            "Dataset loaded.\n",
            "\n",
            "--- Exercise 1 & 2: Grid Search ---\n",
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "Best parameters from Grid Search: {'clf__alpha': np.float64(0.004641588833612777)}\n",
            "What value of alpha did you obtain? 0.004641588833612777\n",
            "What is the model's f1_macro score (Grid Search)? 0.9448299708006562\n",
            "\n",
            "--- Exercise 3: Random Search ---\n",
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "Best parameters from Random Search: {'clf__alpha': np.float64(0.004641588833612782)}\n",
            "What value of alpha did you obtain (Random Search)? 0.004641588833612782\n",
            "What is the model's f1_macro score (Random Search)? 0.9448299708006562\n",
            "Did you get a better f1_macro score compared to grid search in Exercise 2? Both achieved the same f1_macro score.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# 1. Load the dataset (example: 20 Newsgroups)\n",
        "# We'll load a subset for quicker demonstration.\n",
        "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
        "print(\"Loading 20 Newsgroups dataset...\")\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
        "print(\"Dataset loaded.\")\n",
        "\n",
        "X_train, y_train = newsgroups_train.data, newsgroups_train.target\n",
        "X_test, y_test = newsgroups_test.data, newsgroups_test.target\n",
        "\n",
        "# Define the pipeline for text classification\n",
        "pipeline = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('clf', MultinomialNB()),\n",
        "])\n",
        "\n",
        "# Define the parameter grid for alpha\n",
        "# alpha values typically range from 0 (no smoothing) to 1 or more.\n",
        "# We'll use a logarithmic scale for better exploration.\n",
        "param_grid = {\n",
        "    'clf__alpha': np.logspace(-3, 0, 10),  # 10 values between 0.001 and 1\n",
        "}\n",
        "\n",
        "# Use StratifiedKFold for cross-validation\n",
        "# n_splits=5 as requested in the exercise\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "print(\"\\n--- Exercise 1 & 2: Grid Search ---\")\n",
        "\n",
        "# 1. For the Naive Bayes model, use grid search 5-fold cross-validation\n",
        "#    across different values of alpha to find the best model.\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring='f1_macro', n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Best parameters from Grid Search: {grid_search.best_params_}\")\n",
        "best_alpha_grid = grid_search.best_params_['clf__alpha']\n",
        "print(f\"What value of alpha did you obtain? {best_alpha_grid}\")\n",
        "\n",
        "# 2. For the best value of alpha, compute the f1_macro score on the test set.\n",
        "y_pred_grid = grid_search.predict(X_test)\n",
        "f1_macro_grid = f1_score(y_test, y_pred_grid, average='macro')\n",
        "print(f\"What is the model's f1_macro score (Grid Search)? {f1_macro_grid}\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Exercise 3: Random Search ---\")\n",
        "\n",
        "# 3. Repeat Exercise 1 and 2 for random search 5-fold cross-validation\n",
        "#    across different values of alpha. Compute the f1_macro score on the test set.\n",
        "\n",
        "# For random search, we'll draw 10 samples from a broader distribution\n",
        "# or the same distribution, but we typically use more iterations (n_iter).\n",
        "# Let's use 20 iterations for random search to make it distinct from grid search's 10 points.\n",
        "param_distributions = {\n",
        "    'clf__alpha': np.logspace(-3, 0, 100) # Draw from 100 possible values\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(pipeline, param_distributions, n_iter=20, cv=cv,\n",
        "                                   scoring='f1_macro', n_jobs=-1, verbose=1, random_state=42)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Best parameters from Random Search: {random_search.best_params_}\")\n",
        "best_alpha_random = random_search.best_params_['clf__alpha']\n",
        "print(f\"What value of alpha did you obtain (Random Search)? {best_alpha_random}\")\n",
        "\n",
        "y_pred_random = random_search.predict(X_test)\n",
        "f1_macro_random = f1_score(y_test, y_pred_random, average='macro')\n",
        "print(f\"What is the model's f1_macro score (Random Search)? {f1_macro_random}\")\n",
        "\n",
        "# Did you get a better f1_macro score compared to grid search in Exercise 2?\n",
        "if f1_macro_random > f1_macro_grid:\n",
        "    print(f\"Did you get a better f1_macro score compared to grid search in Exercise 2? Yes, Random Search was better.\")\n",
        "elif f1_macro_random < f1_macro_grid:\n",
        "    print(f\"Did you get a better f1_macro score compared to grid search in Exercise 2? No, Grid Search was better.\")\n",
        "else:\n",
        "    print(f\"Did you get a better f1_macro score compared to grid search in Exercise 2? Both achieved the same f1_macro score.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}